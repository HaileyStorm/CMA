# Default Configuration for CMA Model (CMAConfig)

# --- Chunking Configuration ---
# Size of each processing chunk in tokens.
chunk_size: 768
# Minimum percentage gap required between semantic chunks.
semantic_chunking_gap_percentage: 25.0
# Character search ranges (primary, secondary, tertiary) for finding boundary points near ideal chunk splits.
boundary_search_chars: [256, 64, 32]
# Types of boundaries to search for, categorized by importance.
# (This is the effective default set by __post_init__ if not provided)
boundary_types:
  primary:
    - section_break
    - code_block
    - paragraph_break
    - double_line_break
  secondary:
    - sentence_end
    - line_break
  tertiary:
    - clause_end
    - code_line_end
# Ratio of chunk_size to use as a buffer when searching for boundaries.
buffer_ratio: 0.1

# --- Memory Configuration ---
# Maximum size of the compressed memory in tokens.
max_memory_size: 3072
# Size of the memory segment dedicated to the reverse pass (most recent tokens).
reverse_memory_size: 320
# Fraction of max_memory_size allocated during the initial memory write.
initial_write_fraction: 0.6
# Function determining memory growth ("linear", "sqrt", etc.).
memory_growth_function: "linear"
# Maximum input length after which memory size stops growing.
memory_cap_length: 49152
# Whether to share the initial memory state across parallel processing streams (if applicable).
share_initial_memory: false
# Whether to reset the memory state at the beginning of each processing cycle (e.g., for new documents).
reset_memory_on_cycle: true

# --- Reverse Pass Configuration ---
# Maximum number of recent chunks to process in the reverse pass.
reverse_max_chunks: 4
# Token step size for lookahead reverse decay schedule.
lookahead_reverse_decay_step: 0.2
# Decay rate for lookahead reverse attention weights.
lookahead_reverse_decay_rate: 0.5
# Token step size for persistent reverse decay schedule.
persistent_reverse_decay_step: 0.05
# Decay rate for persistent reverse attention weights.
persistent_reverse_decay_rate: 0.1
# Frequency (in tokens) to update persistent reverse memory, if token-based.
persistent_reverse_update_freq_tokens: null # Use null for None
# Frequency (semantic boundary type, e.g., "secondary") to update persistent reverse memory, if semantic-based.
persistent_reverse_update_freq_semantic: null # Use null for None

# --- Model Architecture Configuration ---
# Dimensionality of token embeddings and model hidden states.
embed_dim: 768
# Number of attention heads.
n_heads: 6
# Total number of layers in the model.
# Note: This informs the default layer_structure if layer_structure is null.
n_layers: 12
# Dimensionality of each attention head (typically embed_dim // n_heads).
head_dim: 128
# Explicit definition of layer types and grouping. If null, a default structure is generated.
# (This is the effective default structure generated by __post_init__ for n_layers=12)
layer_structure:
  - type: local_only
  - type: local_only
  - type: local_only
  - type: local_only
  - type: memory_update # Layer index 4
  - type: local_only
  - type: local_only
  - type: local_only
  - type: local_only
  - type: memory_update # Layer index 9
  - type: local_only
  - type: local_only
# Indices of layers where standard attention mechanism should be skipped (e.g., replaced by FFN).
skip_attention_layers: [6]

# --- Control Tokens Configuration ---
# Method for integrating control tokens ("query_fusion", "concat", etc.).
integration_method: "query_fusion"
# Initialization scale for control token embeddings.
ctrl_init_scale: 0.0001

# --- Initialization Configuration ---
# Initialization scale for memory embeddings/parameters.
memory_init_scale: 0.02
# Initial bias value for gating mechanisms (e.g., memory read/write gates).
gate_bias_init: -1.0
# Whether to initialize the final output projection weights to zero.
output_proj_zero_init: true

# --- Adaptive Gating Regularization ---
# Type of regularization for gate activations ("l1", "entropy", or null for none).
gate_regularization_type: null # Use null for None
# Strength of the gate regularization penalty.
gate_regularization_strength: 0.001

# --- Future-Masking Schedule ---
# Progress breakpoints (as fractions of total sequence length) where future masking rate changes.
mask_future_schedule: [0.3, 0.7]
# Rates of future masking applied between breakpoints (initial, middle, final).
mask_future_rates: [0.333, 0.667, 1.0]
# Whether to apply dropout to the future masking mechanism.
enable_mask_future_dropout: true